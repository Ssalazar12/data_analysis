mc_counts = mca.MCA(X_curr)

#plot explained variance as a function of number of factors

N_eig_all = np.linspace(1,35,35).astype(int)
Expl_var_bn = []
Expl_var_bnga = []
for N_eig in N_eig_all:  
    Expl_var_bn.append(np.sum(mc_counts.expl_var(greenacre=False,N=N_eig)))
    Expl_var_bnga.append(np.sum(mc_counts.expl_var(greenacre=True, N=N_eig)))
    
plt.figure(figsize=(8,5))
plt.plot(N_eig_all,Expl_var_bn, label='Benzecri correction')
plt.plot(N_eig_all,Expl_var_bnga,label='Benzecri & Greenacre correction')
plt.legend(loc='lower right')
plt.ylim(0,1.1)
plt.ylabel('explained variance')

#proyect the data into a set of N row factors (distance between rows). Row factors are useful in finding the 
#association between observations
factors = mc_counts.fs_r(N)

#naming the new columns in the df
pd_factors = pd.DataFrame(factors,index = X_curr.index)
fact_cols = ['component'+'_' + str(x) for x in pd_factors.columns ]
pd_factors.columns = fact_cols

#explained variance of the first 10 factors with greenacre correction
mc_counts.expl_var(greenacre=True, N=10)

#get the factors with respect to distance between columns. Columns factors show similarities between features.
mc_counts.fs_c(N=n)

#this function plots the variables with respect to the first 2 factors
# each point in the scatter plot shows a feature

points = pd_column_factors[['factor_0','factor_1']].values
labels = pd_column_factors.index

plt.figure(figsize=(12,12))
plt.margins(0.1)
plt.axhline(0, color='gray')
plt.axvline(0, color='gray')
plt.xlabel('Factor 1')
plt.ylabel('Factor 2')
plt.scatter(points[:,0], points[:,1], s=80, marker='o', c='r', alpha=.5, linewidths=0)

#plot the names of the features
for label, x, y in zip(labels, points[:,0],points[:,1]):
    plt.annotate(label, xy=(x, y), xytext=(x + .03, y + .03))
plt.show()



####################
#MCA AND CLUSTERING#
####################

#elbow method with respect of the column factor
squared_distances = []

ks = range(1,20)
for k in ks:
    km = KMeans(n_clusters = k)
    km = km.fit(pd_column_factors)
    squared_distances.append(km.inertia_)

plt.figure(figsize=(8,8))
plt.plot(ks,squared_distances,'bx-')
plt.xlabel('K')
plt.ylabel('Sum of squared distances')
plt.title('Elbow Method For Optimal K')


#silhouette plot
range_n_clusters = np.arange(2,20)
X = pd_column_factors
score = []
for n_clusters in range_n_clusters:
    
    # Create a subplot with 1 row and 2 columns
    fig, (ax1) = plt.subplots(1, 1)
    fig.set_size_inches(10, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])
    
    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    score.append(silhouette_avg)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])
    
#painting the clusters
km = KMeans(n_clusters = 9)
km = km.fit(pd_column_factors)

pd_column_cluster = pd_column_factors.copy()
pd_column_cluster['cluster'] = km.predict(pd_column_factors)
cluster_labels = pd_column_cluster['cluster'].unique()
colors = ['green','lime','orange','red','navy','grey','cyan','magenta','pink','olive','black','purple']
centroids = km.cluster_centers_

plt.figure(figsize=(10,10))

for i in range(0,len(cluster_labels)):
    cluster_lab = cluster_labels[i]
    color_ = colors[i]
    x_points = pd_column_cluster[pd_column_cluster['cluster']==cluster_lab]['factor_0']
    y_points = pd_column_cluster[pd_column_cluster['cluster']==cluster_lab]['factor_1']
    plt.scatter(x_points,y_points,color=color_)
    

x_points = pd_column_cluster['factor_0'].values
y_points = pd_column_cluster['factor_1'].values

labels = pd_column_cluster.index


for label, x, y in zip(labels, x_points,y_points):
    plt.annotate(label, xy=(x, y), xytext=(x + .003, y + .003))
    
###########################   
#plotting voronoi diagrams#
###########################

#mesh step size
km = KMeans(n_clusters = 7)
km = km.fit(pd_column_factors[['factor_0','factor_1']])

h = .02    
x_min, x_max = pd_column_cluster.iloc[:, 0].min() - 1, pd_column_cluster.iloc[:, 0].max() + 1
y_min, y_max = pd_column_cluster.iloc[:, 1].min() - 1, pd_column_cluster.iloc[:, 1].max() + 1

xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

Z = km.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.figure(figsize=(10,10))
plt.imshow(Z, interpolation='nearest',
           extent=(xx.min(), xx.max(), yy.min(), yy.max()),
           cmap=plt.cm.Paired,
           aspect='auto', origin='lower')


for i in range(0,len(cluster_labels)):
    cluster_lab = cluster_labels[i]
    color_ = colors[i]
    x_points = pd_column_cluster[pd_column_cluster['cluster']==cluster_lab]['factor_0']
    y_points = pd_column_cluster[pd_column_cluster['cluster']==cluster_lab]['factor_1']
    plt.scatter(x_points,y_points,color='black')
    

x_points = pd_column_cluster['factor_0'].values
y_points = pd_column_cluster['factor_1'].values

labels = pd_column_cluster.index


for label, x, y in zip(labels, x_points,y_points):
    plt.annotate(label, xy=(x, y), xytext=(x + .003, y + .003))
    
#select columns with a specific prefix
branch_indices = [i for i,item in enumerate(n) if "branchname" in item]







