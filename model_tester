# scorers
def my_scorer_func(y_true, y_pred):
    tn_fp, fn_tp = confusion_matrix(y_true, y_pred) #(y2.values,f1_array_cvp)
    tn = tn_fp[0]
    fp = tn_fp[1]
    fn = fn_tp[0]
    tp = fn_tp[1]
    
    rec = 0
    rec0 = 0

    rec  = tp / (tp + fn)
    rec0 = tn / (tn + fp)

    score = rec0
    return score


def fnr_score(y, y_pred):
    cm = confusion_matrix(y, y_pred)
    tp = cm[1][1]*1.
    fn = cm[1][0]*1.
    tn = cm[0][0]*1.
    fp = cm[0][1]*1.
    fnr = fn/(tp+fn)
    return fnr

def tnr_score(y, y_pred):
    cm = confusion_matrix(y, y_pred)
    tp = cm[1][1]*1.
    fn = cm[1][0]*1.
    tn = cm[0][0]*1.
    fp = cm[0][1]*1.
    tnr = tn/(tn+fp)
    return tnr


def npv_score(y, y_pred):
    cm = confusion_matrix(y, y_pred)
    fn = cm[1][0]*1.
    tn = cm[0][0]*1.
    npv = tn/(tn+fn)
    return npv
    
def fpr_score(y, y_pred):
    cm = confusion_matrix(y, y_pred)
    tn = cm[0][0]*1.
    fp = cm[0][1]*1.
    fpr = fp/(tn+fp)
    return fpr
    
def my_f1_score(y,y_pred):
    cm = confusion_matrix(y,y_pred)
    tp = float(cm[1][1])
    fp = float(cm[0][1])
    fn = float(cm[1][0])
    precision = tp/(tp + fp)
    recall = tp/(tp + fn)
    f1_score = 2.0 * (precision * recall)/(precision+recall)
    return f1_score
    

def my_recall_score(y,y_pred):
    cm = confusion_matrix(y,y_pred)
    tp = float(cm[1][1])
    fn = float(cm[1][0])
    recall = tp/(tp + fn)
    return recall 
    

tnr_scorer = make_scorer(tnr_score)
fnr_scorer = make_scorer(fnr_score)
fpr_scorer = make_scorer(fpr_score)
npv_scorer = make_scorer(npv_score)
f1_scorer = make_scorer(my_f1_score)
recall_scorer = make_scorer(my_recall_score)
my_scorer = make_scorer(my_scorer_func, greater_is_better=True)


def train_model(train_df, label_columns, model,grid_params):
    #df = dataframe, 
    #test_pipeline = modelo a probar
    #n_splits = numero de folds del k-fold
    #test_pipeline = modelo a probar
    #pipeline_name = nombre de la prueba
    #grid paramas = diccionario con los parametros para grid search ( si se quiere hacer), si no se pone None
    
    start_time = time()
    
    X_train = train_df.copy()
    y_train = label_columns.copy()

    print("Train samples",len(X_train))
    
    if grid_params == None:
    
        model.fit(X_train, y_train.values.ravel())
        y_pred_fs = model.predict(X_train)
        y_probs_fs = model.predict_proba(X_train)
        return_model = model
        
        
    else:
        gs = GridSearchCV(estimator = model, param_grid=grid_params, scoring = my_scorer, cv = 3,\
                         return_train_score=True,iid= True, n_jobs = -1)
        gs.fit(X_train, y_train.values.ravel())
        print(gs.best_params_)
        grid_model = gs.best_estimator_
        y_pred_fs = grid_model.predict(X_train)
        y_probs_fs = grid_model.predict_proba(X_train)
        return_model = gs.best_estimator_


    #confusion matrix
    conf_mat = confusion_matrix(y_train, y_pred_fs)
    t_neg, f_pos, f_neg, t_pos = conf_mat.ravel()
    pre = t_pos/(t_pos+f_pos)
    rec = t_pos/(t_pos+f_neg) 
    fpr = f_pos /(t_neg + f_pos)


    #f1
    f1 = 2.0*(pre*rec/(pre+rec))
    #roc-auc
    #fpr, tpr, thresh = roc_curve(y_train, y_pred_fs)
    
    #roc_auc = auc(fpr, tpr)
    fnr = fnr_score(y_train, y_pred_fs)
    tnr = tnr_score(y_train,y_pred_fs)
    nvp = npv_score(y_train,y_pred_fs)


    prob_df = pd.DataFrame(y_probs_fs)      

           
    print("******* LR + FS ******************")
    print("TPR (recall): ", rec)
    print("FNR: ", fnr)
    print("TNR: ", tnr)
    print("FPR: ", fpr)
    print("NPV: ", nvp) 
    print("F1: ", f1)
    print("PRE: ",pre)
    print("Matrix")
    print(conf_mat)
    
    print("")
             
    print(f'Code took {time() - start_time} seconds')
    
    metrics = {'TPR': [], 'FNR': [], 'TNR': [], 'FPR': [], 'NVP': [], 'F1': [], 'PRE': [], 'confusion_matrix': []}
    metrics['TPR'].append(rec)

    metrics['FNR'].append(fnr)
    
    metrics['TNR'].append(tnr)
    
    metrics['FPR'].append(fpr)
    
    metrics['NVP'].append(nvp)
    
    metrics['F1'].append(f1)
    
    metrics['PRE'].append(pre)
    
    metrics['confusion_matrix'].append(conf_mat)
            
    return prob_df, y_train, return_model, metrics

#modelevaliation
def evaluate_model(model ,X_train, X_test ,y_train, y_test):
    #receives trained model
    best_random_lg = model
    top_features = features

    print("TRAIN RESULTS------------------------")
    y_pred_train = best_random_lg.predict(X_train)
    y_probs_train = best_random_lg.predict_proba(X_train)

    print(confusion_matrix(y_train, y_pred_train))
    f1 = f1_score(y_train,y_pred_train,average='macro')
    print("f1: ",f1)

    plt.figure(figsize=(10,10)) 
    plt.title("ENSAMBLE -> Positive (truth) and negative Distribution LR +fs")
    sns.distplot(y_probs_train[y_train == 1][:,1], color='blue', bins=20,label ='good')# probabilities of the negative examples(bad debt)  P(1|x=0)
    sns.distplot(y_probs_train[y_train == 0][:,1], color= 'red', bins=20,label='bad')# probabilities of the positive examples P(1|x=1)
    plt.legend()
    plt.show()


    print("TEST RESULTS------------------------")
    print("test size: ",len(X_test))
    print("good and bad")
    print(y_test.value_counts())
    y_pred_test = best_random_lg.predict(X_test)
    y_probs_test = best_random_lg.predict_proba(X_test)

    plt.figure(figsize=(10,10)) 

    print(confusion_matrix(y_test, y_pred_test))
    f1 = f1_score(y_test,y_pred_test,average='macro')

    print("f1: ",f1)

    plt.title("ENSAMBLE -> Positive (truth) and negative Distribution LR +fs")
    sns.distplot(y_probs_test[y_test == 1][:,1], color='blue', bins=20,label ='good')# probabilities of the negative examples(bad debt)  P(1|x=0)
    sns.distplot(y_probs_test[y_test == 0][:,1], color= 'red', bins=20,label='bad')# probabilities of the positive examples P(1|x=1)
    plt.legend()

def model_tester(train_data,train_labels,test_data,test_labels ,model_list, grid_params,test_names):
    #model_list is a list containing the instances of each model to test
    #grid_params is a list containing the dictionarys, if no grid params is desired then type None
    #they should be in the same order as the models
    #test_names contains the names for each test
    
    results = []
    
    for i in range(len(model_list)):
        
            metrics = {'model_name': [],'trained_model': [],'train_metrics': [],'test_metrics': []}
            
            print("-------------------------------------------------------")
            print(test_names[i],": train")
            probas_training, y_training ,trained_model, met_train = \
                            train_model(train_data,train_labels, model_list[i], grid_params[i])
            
            print("-------------------------------------------------------")  
            print(test_names[i], " : test")
            probas_test, y_test, met_test = evaluate_model(test_data,test_labels, trained_model)
            
            #train logistic
            fig = plt.figure(figsize=(20,6)) 
            plt.subplot(1, 2, 1)
            fold1 = pd.concat([probas_training.reset_index(drop=True), 
                               y_training.reset_index(drop=True)],axis =1 )
            plt.title("Positive (truth) and negative Distribution Train " + test_names[i])
            sns.distplot(fold1[fold1['our_label']==1][1], color='orange', norm_hist= True,kde=True  )# probabilities of the negative examples(bad debt)  P(1|x=0)
            sns.distplot(fold1[fold1['our_label']==0][1], color= 'blue',norm_hist= True,kde=True  )#

            #test logistic
            plt.subplot(1, 2, 2)
            fold1 = pd.concat([probas_test.reset_index(drop=True), 
                               y_test.reset_index(drop=True)],axis =1 )
            
            plt.title("Positive (truth) and negative Distribution Test"+ test_names[i])
            sns.distplot(fold1[fold1['our_label']==1][1], color='orange', norm_hist= True,kde=True  )# probabilities of the negative examples(bad debt)  P(1|x=0)
            sns.distplot(fold1[fold1['our_label']==0][1], color= 'blue',norm_hist= True,kde=True  )
            fig1 = plt.gcf()
            plt.show()
            plt.draw()
            fig1.savefig(test_names[i]+".pdf",dpi=100)
            
            metrics['model_name'].append(test_names[i])
            metrics['trained_model'].append(trained_model)
            metrics['train_metrics'].append(met_train)
            metrics['test_metrics'].append(met_test)
            
            results.append(metrics)
              
    return results
            

def feature_importance(trained_model, graph_title,train_columns):          
    coeficientes = trained_model.coef_[0]
    matplotlib.rcParams.update({'font.size': 20})

    das_coef = pd.DataFrame({'feature':train_columns,'coef':np.abs(coeficientes)}).sort_values('coef',
                                                                ascending=False)
    #Gr√°fica
    feature_importance = abs(coeficientes)
    feature_importance = 100.0 * (feature_importance / feature_importance.max())
    sorted_idx = np.argsort(feature_importance)
    pos = np.arange(sorted_idx.shape[0]) + .5

    featfig = plt.figure(figsize=(15,25)) 
    featax = featfig.add_subplot(1, 1, 1)
    featax.barh(pos, feature_importance[sorted_idx], align='center')
    featax.set_yticks(pos)
    featax.set_yticklabels(np.array(train_columns)[sorted_idx], fontsize=8)
    featax.set_xlabel('Relative Feature Importance')
    plt.title(graph_title)
    plt.tight_layout()   
    plt.show()
    
    return das_coef.sort_values(by='coef',ascending=False)
    
    
    def test_one_class(df_train,df_test, model ,features,label,inlier_value,outlier_value):
    #includes grid search
    #training_inliers = dataframe with in liers, 
    #test_pipeline = modelo a probar
    df_return = df_train.copy()
    training_inliers = df_train[df_train[label]==inlier_value].copy()
    
    inliers_test = df_test[df_test[label]==inlier_value].copy()
    outliers_test = df_test[df_test[label]==outlier_value].copy()
    
    #fitting model
    model.fit(training_inliers[features])
        
    #training predictions
    df_return['predicted_label'] = model.predict(df_return[features])
    inliers_test['predicted_label'] = model.predict(inliers_test[features])
    outliers_test['predicted_label'] = model.predict(outliers_test[features])
        
    df_return.loc[df_return.predicted_label== -1,'predicted_label'] = outlier_value
    df_return.loc[df_return.predicted_label== 1,'predicted_label'] = inlier_value

    inliers_test.loc[inliers_test.predicted_label==1,'predicted_label'] = inlier_value
    inliers_test.loc[inliers_test.predicted_label==-1,'predicted_label'] = outlier_value

    outliers_test.loc[outliers_test.predicted_label==1,'predicted_label'] = inlier_value
    outliers_test.loc[outliers_test.predicted_label==-1,'predicted_label'] = outlier_value
    
    inliear_acc =  list(inliers_test['predicted_label']).count(0)/inliers_test['predicted_label'].shape[0]
    outlier_acc =  list(outliers_test['predicted_label']).count(1)/outliers_test['predicted_label'].shape[0]
    
    both_matrices = confusion_matrix(inliers_test['predicted_label'], inliers_test['our_label']) + \
                confusion_matrix(outliers_test['predicted_label'], outliers_test['our_label'])
    
    predicted_all = pd.concat([inliers_test[['predicted_label','our_label']] , 
                               outliers_test[['predicted_label','our_label']]] , axis = 0)
    
    f1_m = f1_score(predicted_all['our_label'],predicted_all['predicted_label'],average='macro')
    
    return inliear_acc, outlier_acc, both_matrices,f1_m

######## test a model many times------------------------

f1s_our = []
f1s_johan = []
f1s_nodrop = []

for i in range(100):
    
    seed = np.random.randint(10,500)
    
    label='bad_debt'
    _, y_probs_input_fixcost, y_test = training_with_best(train_data,features_input_fixcost,label,
                                                     'INPUT & FIXCOST',input_fix_our_params,seed)
                                                                
    _, y_probs_input_cashin, y_test = training_with_best(train_data,features_input_cashin,label,
                                    'INPUT & CASHIN',input_cash_our_params,seed)

    y_probs = (y_probs_input_fixcost + y_probs_input_cashin ) / 2 
    y_pred_ens = np.where(y_probs > 0.55, 1, 0)

    print(confusion_matrix(y_test, y_pred_ens[:,1], labels=label_values.sort()))
    
    f1 = f1_score(y_test,y_pred_ens[:,1],average='macro')
    f1s_our.append(f1)
    
    
    #-----------------------------------------------------------------------------------
    
    label = 'new_johan_label'
    
    features_all = [ c for c in features for f in filter_all if f in c]
                                                                
    _, y_probs_all, y_test = training_with_best(train_data,features_all,label,
                                                     'ALL',all_johan_params,seed)

    y_probs = y_probs_all
    y_pred_ens = np.where(y_probs > 0.452, 1, 0)

    print(confusion_matrix(y_test, y_pred_ens[:,1], labels=label_values.sort()))
    
    f1 = f1_score(y_test,y_pred_ens[:,1],average='macro')
    f1s_johan.append(f1)
    
    #------Without dropping loans-------------------------------------------------------#
    label = 'new_johan_label'
    
    features_all = [ c for c in features for f in filter_all if f in c]
                                                                
    _, y_probs_all, y_test = train_best_no_drop(train_data_all,features_all,label,
                                                     'ALL',no_drop_all_johan_params,seed)

    y_probs = y_probs_all
    y_pred_ens = np.where(y_probs > 0.532, 1, 0)

    print(confusion_matrix(y_test, y_pred_ens[:,1], labels=label_values.sort()))
    
    f1 = f1_score(y_test,y_pred_ens[:,1],average='macro')
    f1s_nodrop.append(f1)
    

def get_best_model(data,features,label, model,grid,test_size,outlier_value,inlier_value,seed):
    #data son todos los datos
    #el mejor modelo es aquel con mayor recall
    #model es la instancia del modelo a probar
    #grid son los parametros que hay que probar
    #outlier_value, inlier_value: indican cual valore del label corresponde a inlier y outlier
    #retorna la matriz de confusi√≥n, el recall y los par√°metros del mejor modelo
    
    df_train, y_train, df_test, y_test = modified_train_test_split(data, label,test_size,seed,features)
    
    matrices = []
    good_accs = []
    bad_accs = []
    params = []
    recalls = []
    precisions = []
    f1s = []
    
    for z in ParameterGrid(grid):
        model.set_params(**z)
        good_acc, bad_acc, conf_matrix, y_pred, y_true = test_one_class(df_train,df_test, model ,features ,label,
                                                        inlier_value,outlier_value)
        
        rec = conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[1,0])
        pre = conf_matrix[1,1]/(conf_matrix[1,1]+conf_matrix[0,1])
        f1 = f1_score(y_true,y_pred,average='macro')
        
        matrices.append(conf_matrix)
        good_accs.append(good_acc)
        bad_accs.append(bad_acc)
        params.append(z)
        recalls.append(rec)
        precisions.append(pre)
        f1s.append(f1)
        
    max_i = f1s.index(max(f1s))
    print(params[max_i])
    print("Confusion matrix: ")
    print(matrices[max_i])
    print("recall: ", recalls[max_i])
    print("f1: ", f1s[max_i])
    
    return params[max_i],recalls[max_i], matrices[max_i], precisions[max_i],f1s[max_i]
    
    
    ## PIPELINES
    
    #steps definitions

classifier_ = SGDClassifier()
feature_map = RBFSampler(gamma=.2, random_state = 42)
sampler = SMOTE(random_state=42)
scaler = StandardScaler()
params = svc_random
feat = top_features

#function work

steps_ = [('scaler',scaler),('sampler',sampler),('classifier',classifier_)]

pipeline = Pipeline(steps_)

scoring = ['f1_macro','precision']

r_search = RandomizedSearchCV(estimator = pipeline, param_distributions = params, n_iter = 100,  scoring = scoring,
                                  refit = 'precision',cv = 10, random_state=42, n_jobs = -1,iid = True)

X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.35, random_state=42)


r_search.fit(X_train,y_train)

r_search.best_estimator_

y_train_predict = r_search.best_estimator_.predict(X_train)

print(r_search.best_params_)
print("f1_score: ", f1_score(y_train, y_train_predict, average='macro'))
print(classification_report(y_train, y_train_predict ))
print(confusion_matrix(y_train, y_train_predict))

best_svc = r_search.best_estimator_
evaluate_model(r_search.best_estimator_ ,X_train, X_test ,y_train, y_test)

####
## TEST A MODEL IN ONE CELL ##########################
####
X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y , test_size=0.3, random_state=42)

#define ensemble
model1 = LogisticRegression(random_state=1)
model2 = DecisionTreeClassifier(random_state=1)
model3 = SGDClassifier()

model = VotingClassifier(estimators=[('lr', model1), ('dt', model2), ('svc',model3)], voting='soft')
scaler = StandardScaler()


#define pipe line
steps_ = [('scaler',scaler),('classifier',model)]
pipeline = Pipeline(steps_)

classifier = pipeline
params = sc_logic_tree_scv_params
scoring = 'f1_macro'

voting_search = RandomizedSearchCV(estimator = classifier, param_distributions = params, n_iter = 100, 
                                   scoring = scoring,cv = 2, random_state=42, n_jobs = -1,iid = True)
voting_search.fit(X_train,y_train)

y_train_predict = voting_search.predict(X_train)

print(voting_search.best_params_)
print("f1_score: ", f1_score(y_train, y_train_predict, average='macro'))
print(classification_report(y_train, y_train_predict ))
print(confusion_matrix(y_train, y_train_predict))


evaluate_model(voting_search.best_estimator_,top_features ,X_train, X_test ,y_train, y_test)



#############################
#####Feature extraction#####
############################
weights_rf =rf_cl.feature_importances_

rf_drop_importance= pd.DataFrame({'feature': extended_features,'coef': weights_rf}).sort_values(by='coef',ascending=False)
rf_drop_importance.head(40)
