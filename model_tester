# scorers
def my_scorer_func(y_true, y_pred):
    tn_fp, fn_tp = confusion_matrix(y_true, y_pred) #(y2.values,f1_array_cvp)
    tn = tn_fp[0]
    fp = tn_fp[1]
    fn = fn_tp[0]
    tp = fn_tp[1]
    
    rec = 0
    rec0 = 0

    rec  = tp / (tp + fn)
    rec0 = tn / (tn + fp)

    score = rec0
    return score


def fnr_score(y, y_pred):
    cm = confusion_matrix(y, y_pred)
    tp = cm[1][1]*1.
    fn = cm[1][0]*1.
    tn = cm[0][0]*1.
    fp = cm[0][1]*1.
    fnr = fn/(tp+fn)
    return fnr

def tnr_score(y, y_pred):
    cm = confusion_matrix(y, y_pred)
    tp = cm[1][1]*1.
    fn = cm[1][0]*1.
    tn = cm[0][0]*1.
    fp = cm[0][1]*1.
    tnr = tn/(tn+fp)
    return tnr


def npv_score(y, y_pred):
    cm = confusion_matrix(y, y_pred)
    fn = cm[1][0]*1.
    tn = cm[0][0]*1.
    npv = tn/(tn+fn)
    return npv
    
def fpr_score(y, y_pred):
    cm = confusion_matrix(y, y_pred)
    tn = cm[0][0]*1.
    fp = cm[0][1]*1.
    fpr = fp/(tn+fp)
    return fpr
    
def my_f1_score(y,y_pred):
    cm = confusion_matrix(y,y_pred)
    tp = float(cm[1][1])
    fp = float(cm[0][1])
    fn = float(cm[1][0])
    precision = tp/(tp + fp)
    recall = tp/(tp + fn)
    f1_score = 2.0 * (precision * recall)/(precision+recall)
    return f1_score
    

def my_recall_score(y,y_pred):
    cm = confusion_matrix(y,y_pred)
    tp = float(cm[1][1])
    fn = float(cm[1][0])
    recall = tp/(tp + fn)
    return recall 
    

tnr_scorer = make_scorer(tnr_score)
fnr_scorer = make_scorer(fnr_score)
fpr_scorer = make_scorer(fpr_score)
npv_scorer = make_scorer(npv_score)
f1_scorer = make_scorer(my_f1_score)
recall_scorer = make_scorer(my_recall_score)
my_scorer = make_scorer(my_scorer_func, greater_is_better=True)


def train_model(train_df, label_columns, model,grid_params):
    #df = dataframe, 
    #test_pipeline = modelo a probar
    #n_splits = numero de folds del k-fold
    #test_pipeline = modelo a probar
    #pipeline_name = nombre de la prueba
    #grid paramas = diccionario con los parametros para grid search ( si se quiere hacer), si no se pone None
    
    start_time = time()
    
    X_train = train_df.copy()
    y_train = label_columns.copy()

    print("Train samples",len(X_train))
    
    if grid_params == None:
    
        model.fit(X_train, y_train.values.ravel())
        y_pred_fs = model.predict(X_train)
        y_probs_fs = model.predict_proba(X_train)
        return_model = model
        
        
    else:
        gs = GridSearchCV(estimator = model, param_grid=grid_params, scoring = my_scorer, cv = 3,\
                         return_train_score=True,iid= True, n_jobs = -1)
        gs.fit(X_train, y_train.values.ravel())
        print(gs.best_params_)
        grid_model = gs.best_estimator_
        y_pred_fs = grid_model.predict(X_train)
        y_probs_fs = grid_model.predict_proba(X_train)
        return_model = gs.best_estimator_


    #confusion matrix
    conf_mat = confusion_matrix(y_train, y_pred_fs)
    t_neg, f_pos, f_neg, t_pos = conf_mat.ravel()
    pre = t_pos/(t_pos+f_pos)
    rec = t_pos/(t_pos+f_neg) 
    fpr = f_pos /(t_neg + f_pos)


    #f1
    f1 = 2.0*(pre*rec/(pre+rec))
    #roc-auc
    #fpr, tpr, thresh = roc_curve(y_train, y_pred_fs)
    
    #roc_auc = auc(fpr, tpr)
    fnr = fnr_score(y_train, y_pred_fs)
    tnr = tnr_score(y_train,y_pred_fs)
    nvp = npv_score(y_train,y_pred_fs)


    prob_df = pd.DataFrame(y_probs_fs)      

           
    print("******* LR + FS ******************")
    print("TPR (recall): ", rec)
    print("FNR: ", fnr)
    print("TNR: ", tnr)
    print("FPR: ", fpr)
    print("NPV: ", nvp) 
    print("F1: ", f1)
    print("PRE: ",pre)
    print("Matrix")
    print(conf_mat)
    
    print("")
             
    print(f'Code took {time() - start_time} seconds')
    
    metrics = {'TPR': [], 'FNR': [], 'TNR': [], 'FPR': [], 'NVP': [], 'F1': [], 'PRE': [], 'confusion_matrix': []}
    metrics['TPR'].append(rec)

    metrics['FNR'].append(fnr)
    
    metrics['TNR'].append(tnr)
    
    metrics['FPR'].append(fpr)
    
    metrics['NVP'].append(nvp)
    
    metrics['F1'].append(f1)
    
    metrics['PRE'].append(pre)
    
    metrics['confusion_matrix'].append(conf_mat)
            
    return prob_df, y_train, return_model, metrics

def evaluate_model(feature_colums ,label_columns, model):
    #df = dataframe, 
    #test_pipeline = modelo a probar
    #n_splits = numero de folds del k-fold
    #test_pipeline = modelo a probar
    #pipeline_name = nombre de la prueba
    start_time = time()
    X_test = feature_colums.copy()
    y_test = label_columns.copy()

    print("Test samples",len(X_test))
    y_pred_fs = model.predict(X_test)
    y_probs_fs = model.predict_proba(X_test)

    #confusion matrix
    conf_mat = confusion_matrix(y_test, y_pred_fs)
    t_neg, f_pos, f_neg, t_pos = conf_mat.ravel()
    pre = t_pos/(t_pos+f_pos)
    rec = t_pos/(t_pos+f_neg) 
    fpr = f_pos /(t_neg + f_pos)

    #f1
    f1 = 2.0*(pre*rec/(pre+rec))
    #roc-auc
    #fpr, tpr, thresh = roc_curve(y_test, y_pred_fs)
    #roc_auc = auc(fpr, tpr)
    fnr = fnr_score(y_test, y_pred_fs)
    tnr = tnr_score(y_test,y_pred_fs)
    nvp = npv_score(y_test,y_pred_fs)

    prob_df = pd.DataFrame(y_probs_fs)      
          
    print("******* LR + fs ******************")
    print("TPR (recall): ", rec)
    print("FNR: ", fnr)
    print("TNR: ", tnr)
    print("FPR: ", fpr)
    print("NPV: ", nvp) 
    print("F1: ", f1)
    print("PRE: ",pre)
    print("Matrix")
    print(conf_mat)
    
    print("")
                     
    print(f'Code took {time() - start_time} seconds')
    
    metrics = {'TPR': [], 'FNR': [], 'TNR': [], 'FPR': [], 'NVP': [], 'F1': [], 'PRE': [], 'confusion_matrix': []}
    metrics['TPR'].append(rec)

    metrics['FNR'].append(fnr)
    
    metrics['TNR'].append(tnr)
    
    metrics['FPR'].append(fpr)
    
    metrics['NVP'].append(nvp)
    
    metrics['F1'].append(f1)
    
    metrics['PRE'].append(pre)
    
    metrics['confusion_matrix'].append(conf_mat)
    
    return prob_df, y_test, metrics
    
    
    def modified_train_test_split(dataset, dataset_label_col,testsize):
    #dataset incluye TODAS las columnas
    #dataset_label_col solo las labels
    #se asegura que no queden clientes en train y test al mismo tiempo
    df_train, df_test, y_train, y_test = train_test_split(dataset, dataset_label_col, 
                                                      test_size=testsize, random_state=41)
    #asegurandose que no queden clientes en training y test al mismo tiempo
    #gets customer ids for each set
    train_customers = df_train['customer_id'].unique()
    test_customers = df_test['customer_id']. unique()
    bool_mask = np.isin(train_customers,test_customers)
    
    #customer_ids that intersect
    intersect = train_customers[bool_mask]
    #getting the indices for the intersection in x_train 
    inter_indices = df_train[df_train['customer_id'].isin(intersect)].index.values.astype(int)
    #Delete rows with index label from train 
    df_train_mod = df_train.drop(inter_indices,axis=0)
    #grab the rows with those indices
    df_intersection = df_train.loc[inter_indices]
    #add them to test
    df_test_mod = pd.concat([df_test,df_intersection], axis = 0)
    
    #toma los customer ids para cada uno
    training_ids = df_train_mod['customer_id'].unique()
    test_ids = df_test_mod['customer_id'].unique()

    #important to reset de indices
    training_set = dataset[dataset['customer_id'].isin(training_ids)]
    training_set = training_set.reset_index(drop=True)
    test_set = dataset[dataset['customer_id'].isin(test_ids)]
    test_set = test_set.reset_index(drop=True)

    df_train = training_set.drop(non_features, axis=1)
    y_train = training_set['our_label']
    df_test = test_set.drop(non_features, axis=1)
    y_test = test_set['our_label']

    print("training set size: ", len(df_train))
    print("test set size: ",len(df_test))
    
    return df_train, y_train, df_test, y_test

def model_tester(train_data,train_labels,test_data,test_labels ,model_list, grid_params,test_names):
    #model_list is a list containing the instances of each model to test
    #grid_params is a list containing the dictionarys, if no grid params is desired then type None
    #they should be in the same order as the models
    #test_names contains the names for each test
    
    results = []
    
    for i in range(len(model_list)):
        
            metrics = {'model_name': [],'trained_model': [],'train_metrics': [],'test_metrics': []}
            
            print("-------------------------------------------------------")
            print(test_names[i],": train")
            probas_training, y_training ,trained_model, met_train = \
                            train_model(train_data,train_labels, model_list[i], grid_params[i])
            
            print("-------------------------------------------------------")  
            print(test_names[i], " : test")
            probas_test, y_test, met_test = evaluate_model(test_data,test_labels, trained_model)
            
            #train logistic
            fig = plt.figure(figsize=(20,6)) 
            plt.subplot(1, 2, 1)
            fold1 = pd.concat([probas_training.reset_index(drop=True), 
                               y_training.reset_index(drop=True)],axis =1 )
            plt.title("Positive (truth) and negative Distribution Train " + test_names[i])
            sns.distplot(fold1[fold1['our_label']==1][1], color='orange', norm_hist= True,kde=True  )# probabilities of the negative examples(bad debt)  P(1|x=0)
            sns.distplot(fold1[fold1['our_label']==0][1], color= 'blue',norm_hist= True,kde=True  )#

            #test logistic
            plt.subplot(1, 2, 2)
            fold1 = pd.concat([probas_test.reset_index(drop=True), 
                               y_test.reset_index(drop=True)],axis =1 )
            
            plt.title("Positive (truth) and negative Distribution Test"+ test_names[i])
            sns.distplot(fold1[fold1['our_label']==1][1], color='orange', norm_hist= True,kde=True  )# probabilities of the negative examples(bad debt)  P(1|x=0)
            sns.distplot(fold1[fold1['our_label']==0][1], color= 'blue',norm_hist= True,kde=True  )
            fig1 = plt.gcf()
            plt.show()
            plt.draw()
            fig1.savefig(test_names[i]+".pdf",dpi=100)
            
            metrics['model_name'].append(test_names[i])
            metrics['trained_model'].append(trained_model)
            metrics['train_metrics'].append(met_train)
            metrics['test_metrics'].append(met_test)
            
            results.append(metrics)
              
    return results
            

def feature_importance(trained_model, graph_title,train_columns):          
    coeficientes = trained_model.coef_[0]
    matplotlib.rcParams.update({'font.size': 20})

    das_coef = pd.DataFrame({'feature':train_columns,'coef':np.abs(coeficientes)}).sort_values('coef',
                                                                ascending=False)
    #Gráfica
    feature_importance = abs(coeficientes)
    feature_importance = 100.0 * (feature_importance / feature_importance.max())
    sorted_idx = np.argsort(feature_importance)
    pos = np.arange(sorted_idx.shape[0]) + .5

    featfig = plt.figure(figsize=(15,25)) 
    featax = featfig.add_subplot(1, 1, 1)
    featax.barh(pos, feature_importance[sorted_idx], align='center')
    featax.set_yticks(pos)
    featax.set_yticklabels(np.array(train_columns)[sorted_idx], fontsize=8)
    featax.set_xlabel('Relative Feature Importance')
    plt.title(graph_title)
    plt.tight_layout()   
    plt.show()
    
    return das_coef.sort_values(by='coef',ascending=False)
    
    
    def test_one_class(df_train,df_test, model ,features,label,inlier_value,outlier_value):
    #includes grid search
    #training_inliers = dataframe with in liers, 
    #test_pipeline = modelo a probar
    df_return = df_train.copy()
    training_inliers = df_train[df_train[label]==inlier_value].copy()
    
    inliers_test = df_test[df_test[label]==inlier_value].copy()
    outliers_test = df_test[df_test[label]==outlier_value].copy()
    
    #fitting model
    model.fit(training_inliers[features])
        
    #training predictions
    df_return['predicted_label'] = model.predict(df_return[features])
    inliers_test['predicted_label'] = model.predict(inliers_test[features])
    outliers_test['predicted_label'] = model.predict(outliers_test[features])
        
    df_return.loc[df_return.predicted_label== -1,'predicted_label'] = outlier_value
    df_return.loc[df_return.predicted_label== 1,'predicted_label'] = inlier_value

    inliers_test.loc[inliers_test.predicted_label==1,'predicted_label'] = inlier_value
    inliers_test.loc[inliers_test.predicted_label==-1,'predicted_label'] = outlier_value

    outliers_test.loc[outliers_test.predicted_label==1,'predicted_label'] = inlier_value
    outliers_test.loc[outliers_test.predicted_label==-1,'predicted_label'] = outlier_value
    
    inliear_acc =  list(inliers_test['predicted_label']).count(0)/inliers_test['predicted_label'].shape[0]
    outlier_acc =  list(outliers_test['predicted_label']).count(1)/outliers_test['predicted_label'].shape[0]
    
    both_matrices = confusion_matrix(inliers_test['predicted_label'], inliers_test['our_label']) + \
                confusion_matrix(outliers_test['predicted_label'], outliers_test['our_label'])
    
    predicted_all = pd.concat([inliers_test[['predicted_label','our_label']] , 
                               outliers_test[['predicted_label','our_label']]] , axis = 0)
    
    f1_m = f1_score(predicted_all['our_label'],predicted_all['predicted_label'],average='macro')
    
    return inliear_acc, outlier_acc, both_matrices,f1_m

